{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Piyush150398/Fine-Tuning-LLM/blob/main/FineTuning_using_SFT_for_Financial_Sentiment_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxFEy9lIlRIt"
      },
      "source": [
        "Lets Import the Data set from DeepLake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KBV0uiobFdF",
        "outputId": "2d87cb60-b289-4c7e-cbd1-e74d789c0185"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: deeplake in /usr/local/lib/python3.10/dist-packages (3.8.11)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.10/dist-packages (0.7.4)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.25.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deeplake) (1.23.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from deeplake) (9.4.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (from deeplake) (1.33.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from deeplake) (8.1.7)\n",
            "Requirement already satisfied: pathos in /usr/local/lib/python3.10/dist-packages (from deeplake) (0.3.1)\n",
            "Requirement already satisfied: humbug>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from deeplake) (0.3.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deeplake) (4.66.1)\n",
            "Requirement already satisfied: lz4 in /usr/local/lib/python3.10/dist-packages (from deeplake) (4.3.2)\n",
            "Requirement already satisfied: pyjwt in /usr/lib/python3/dist-packages (from deeplake) (2.3.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from deeplake) (1.10.13)\n",
            "Requirement already satisfied: libdeeplake==0.0.92 in /usr/local/lib/python3.10/dist-packages (from deeplake) (0.0.92)\n",
            "Requirement already satisfied: aioboto3>=10.4.0 in /usr/local/lib/python3.10/dist-packages (from deeplake) (12.1.0)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from deeplake) (1.5.8)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from libdeeplake==0.0.92->deeplake) (0.3.7)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from trl) (2.1.0+cu118)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from trl) (2.15.0)\n",
            "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.10/dist-packages (from trl) (0.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: aiobotocore[boto3]==2.8.0 in /usr/local/lib/python3.10/dist-packages (from aioboto3>=10.4.0->deeplake) (2.8.0)\n",
            "Requirement already satisfied: botocore<1.33.2,>=1.32.4 in /usr/local/lib/python3.10/dist-packages (from aiobotocore[boto3]==2.8.0->aioboto3>=10.4.0->deeplake) (1.33.1)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.7.4.post0 in /usr/local/lib/python3.10/dist-packages (from aiobotocore[boto3]==2.8.0->aioboto3>=10.4.0->deeplake) (3.9.1)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore[boto3]==2.8.0->aioboto3>=10.4.0->deeplake) (1.14.1)\n",
            "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from aiobotocore[boto3]==2.8.0->aioboto3>=10.4.0->deeplake) (0.11.0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3->deeplake) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from boto3->deeplake) (0.8.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (2.1.0)\n",
            "Requirement already satisfied: docstring-parser>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (0.15)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (13.7.0)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (1.6.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (9.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (0.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (0.70.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: ppft>=1.7.6.7 in /usr/local/lib/python3.10/dist-packages (from pathos->deeplake) (1.7.6.7)\n",
            "Requirement already satisfied: pox>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from pathos->deeplake) (0.3.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.7.4.post0->aiobotocore[boto3]==2.8.0->aioboto3>=10.4.0->deeplake) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.7.4.post0->aiobotocore[boto3]==2.8.0->aioboto3>=10.4.0->deeplake) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.7.4.post0->aiobotocore[boto3]==2.8.0->aioboto3>=10.4.0->deeplake) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.7.4.post0->aiobotocore[boto3]==2.8.0->aioboto3>=10.4.0->deeplake) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.7.4.post0->aiobotocore[boto3]==2.8.0->aioboto3>=10.4.0->deeplake) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.7.4.post0->aiobotocore[boto3]==2.8.0->aioboto3>=10.4.0->deeplake) (4.0.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.33.2,>=1.32.4->aiobotocore[boto3]==2.8.0->aioboto3>=10.4.0->deeplake) (2.8.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.16.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->trl) (2.1.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2023.3.post1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.33.2,>=1.32.4->aiobotocore[boto3]==2.8.0->aioboto3>=10.4.0->deeplake) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install deeplake trl peft transformers accelerate wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TP0XUoOBlYlH",
        "outputId": "57507f6d-e704-4a2c-abfc-dddf371abc40"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Opening dataset in read-only mode as you don't have write permissions.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "-"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/genai360/FingGPT-sentiment-train-set\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "-"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hub://genai360/FingGPT-sentiment-train-set loaded successfully.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Opening dataset in read-only mode as you don't have write permissions.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\\"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/genai360/FingGPT-sentiment-valid-set\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\\"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hub://genai360/FingGPT-sentiment-valid-set loaded successfully.\n",
            "\n",
            "Dataset(path='hub://genai360/FingGPT-sentiment-train-set', read_only=True, tensors=['input', 'instruction', 'output'])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r \r\r\r"
          ]
        }
      ],
      "source": [
        "import deeplake\n",
        "\n",
        "ds = deeplake.load('hub://genai360/FingGPT-sentiment-train-set')\n",
        "\n",
        "ds_valid = deeplake.load('hub://genai360/FingGPT-sentiment-valid-set') # validation set\n",
        "\n",
        "print(ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ayguy8aulp2G",
        "outputId": "f2f5073d-067c-46f4-9c95-59f3eef3ac41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset(path='hub://genai360/FingGPT-sentiment-train-set', read_only=True, tensors=['input', 'instruction', 'output'])\n",
            "\n",
            "   tensor      htype     shape      dtype  compression\n",
            "   -------    -------   -------    -------  ------- \n",
            "    input      text    (20000, 1)    str     None   \n",
            " instruction   text    (20000, 1)    str     None   \n",
            "   output      text    (20000, 1)    str     None   \n"
          ]
        }
      ],
      "source": [
        "ds.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        },
        "id": "VNmmbfRhnJIm",
        "outputId": "1a0b1b4f-8f17-4e46-bc73-ddff3fdbd2e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HINT: Please forward the port - 40855 to your local machine, if you are running on the cloud.\n",
            " * Serving Flask app 'dataset_visualizer'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"90%\"\n",
              "            height=\"800\"\n",
              "            src=\"https://app.activeloop.ai/visualizer/hub?url=hub://genai360/FingGPT-sentiment-train-set&token=eyJhbGciOiJIUzUxMiIsImlhdCI6MTcwMjIxMDE2MSwiZXhwIjoxNzA1ODEwMTYxfQ.eyJpZCI6InB1YmxpYyJ9.h68xhOeIV0QgyKlt3p80k4SuSeA8dV_XFJ3omSuKZwRsZvtkaG7anAThgVN4d_o-3z9Z43whmtKQariEUZJ7KA\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x790dbe55b3d0>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "ds.visualize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0WMC1H_q_iN"
      },
      "source": [
        "# **Lets Look into data-set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRINbTCyntYH",
        "outputId": "88c88d98-5162-43a8-b3c0-23bd9185f0a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In December , Amer announced the dismissal of CEO Roger Talermo , who had headed the company since 2006 .\n",
            "What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}\n",
            "neutral\n"
          ]
        }
      ],
      "source": [
        "# Input tweet\n",
        "print(ds.input[7].text())\n",
        "\n",
        "# Input Instruction\n",
        "print(ds.instruction[7].text())\n",
        "\n",
        "# Output\n",
        "print(ds.output[7].text())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7V6E19HloSul",
        "outputId": "5425aef6-a97a-4bd9-afd1-18542e8c09c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the sentiment of this tweet? Please choose an answer from {negative/neutral/positive}\n",
            "\n",
            "Content: Tesla Motors recalls 2,700 Model X SUVs https://t.co/BFWS3DbM0U $TSLA\n",
            "\n",
            "Sentiment: negative\n"
          ]
        }
      ],
      "source": [
        "def prepare_sample_text(example):\n",
        "    \"\"\"Prepare the text from a sample of the dataset.\"\"\"\n",
        "    text = f\"{example['instruction'].text()}\\n\\nContent: {example['input'].text()}\\n\\nSentiment: {example['output'].text()}\"\n",
        "    return text\n",
        "\n",
        "print(prepare_sample_text(ds[:,0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4Zy_JU_xsMo",
        "outputId": "9f6ce83f-3ea7-4420-a050-50b417425358"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/utils.py:548: UserWarning: The passed formatting_func has more than one argument. Usually that function should have a single argument `example` which corresponds to the dictionary returned by each element of the dataset. Make sure you know what you are doing.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([  35, 1313,    2,  ...,    2,    2, 2264]), 'labels': tensor([  35, 1313,    2,  ...,    2,    2, 2264])}\n"
          ]
        }
      ],
      "source": [
        "# Load the tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n",
        "\n",
        "# create the constant length dataset\n",
        "from trl.trainer import ConstantLengthDataset\n",
        "\n",
        "# Need to import the constant length dataset for training\n",
        "train_dataset = ConstantLengthDataset(\n",
        "    tokenizer,   # select tokenizer\n",
        "    ds,          # select dataset\n",
        "    formatting_func=prepare_sample_text, # select formatting function\n",
        "    infinite=True,\n",
        "    seq_length=1024 # which means the input sequences fed to the model during training will have a length of 1024 tokens\n",
        ")\n",
        "\n",
        "eval_dataset = ConstantLengthDataset(\n",
        "    tokenizer,\n",
        "    ds_valid,\n",
        "    formatting_func=prepare_sample_text,\n",
        "    seq_length=1024\n",
        ")\n",
        "\n",
        "# Show one sample from train set\n",
        "iterator = iter(train_dataset)\n",
        "sample = next(iterator)\n",
        "print(sample)\n",
        "train_dataset.start_iteration = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rT51cW9QjX-c"
      },
      "outputs": [],
      "source": [
        "# If having following error then run this cell\n",
        "# ImportError: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`\n",
        "import os\n",
        "os._exit(00)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3UDXp8zbWp3"
      },
      "outputs": [],
      "source": [
        "# Define LoRA configuration\n",
        "from peft import LoraConfig\n",
        "\n",
        "loar_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Define Traning Arguments\n",
        "# The TrainingArguments class is part of the Hugging Face Transformers library and is commonly used to configure training settings for machine learning models.\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./OPT-fine_tuned-FinGPT-CPU\",  # Directory where model checkpoints and predictions will be saved\n",
        "    dataloader_drop_last=True,  # Drops the last incomplete batch if the dataset size is not divisible by the batch size\n",
        "    evaluation_strategy=\"epoch\",  # Evaluate every epoch\n",
        "    save_strategy=\"epoch\",  # Save model every epoch\n",
        "    num_train_epochs=10,  # Total number of training epochs\n",
        "    logging_steps=5,  # Log training information every 5 steps\n",
        "    per_device_train_batch_size=12,  # Batch size per GPU/TPU core/CPU for training\n",
        "    per_device_eval_batch_size=12,  # Batch size per GPU/TPU core/CPU for evaluation\n",
        "    learning_rate=1e-4,  # Learning rate for the optimizer\n",
        "    lr_scheduler_type=\"cosine\",  # Cosine learning rate scheduler\n",
        "    warmup_steps=100,  # Number of warmup steps for the learning rate scheduler\n",
        "    gradient_accumulation_steps=1,  # Number of steps for gradient accumulation before performing a backward/update pass\n",
        "    gradient_checkpointing=False,  # Use gradient checkpointing to reduce memory usage during training\n",
        "    fp16=False,  # Use 16-bit (half-precision) floating-point precision for training\n",
        "    bf16=True,  # Use 16-bit (bfloat16) floating-point precision for training\n",
        "    weight_decay=0.05,  # Strength of weight decay regularization\n",
        "    ddp_find_unused_parameters=False,  # Distributed Data Parallel (DDP) option to find and report unused parameters\n",
        "    run_name=\"OPT-fine_tuned-FinGPT-CPU\",  # Name of the training run (for logging purposes)\n",
        "    report_to=\"wandb\",  # Report training progress to Weights & Biases (wandb)\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAs_cNZIkEoY"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"facebook/opt-1.3b\", torch_dtype=torch.bfloat16\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgDIhkXHmtM1"
      },
      "source": [
        "### This code is preparing a pre-trained language model for further training by freezing most of its parameters, ensuring stability for certain parameters, enabling gradient checkpointing, and customizing the output layer for potential downstream tasks. The focus seems to be on training adapters or other task-specific components while keeping the majority of the model fixed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bx50jwNslZT8"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "# Loop through all parameters in the model\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False  # Freeze the model parameters - train adapters later (This is done to prevent these parameters from being updated during training, as indicated by the comment.)\n",
        "    if param.ndim == 1:\n",
        "        # Cast small parameters (e.g., layernorm) to fp32 for stability\n",
        "        param.data = param.data.to(torch.float32)\n",
        "\n",
        "# Enable gradient checkpointing to reduce the number of stored activations\n",
        "# a technique used to reduce the memory requirements during backpropagation by recomputing intermediate activations as needed.\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# Enable input gradients to be required (useful for training adapters later)\n",
        "\"\"\"This line enables input gradients to be required, which is useful for training adapters later. Adapters are additional small neural networks that can be trained on specific downstream tasks while keeping the rest of the pre-trained model frozen.\"\"\"\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "# Define a custom module to cast the output to fp32\n",
        "class CastOutputToFloat(nn.Sequential):\n",
        "    def forward(self, x):\n",
        "        return super().forward(x).to(torch.float32)\n",
        "\n",
        "# Replace the lm_head (language modeling head) of the model with the custom CastOutputToFloat module\n",
        "model.lm_head = CastOutputToFloat(model.lm_head)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jZqPMPJm-Ca"
      },
      "source": [
        "### Now, connect the model, dataset, training arguments, and Lora config together using the SFTTrainer class to start the training process by invoking the .train() method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "5Odwaa6qnCnS",
        "outputId": "a04eef0d-e11a-4b53-8996-a1f64a68d797"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training...\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20231210_121348-0mqj9454</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/piyushsonawane10/huggingface/runs/0mqj9454' target=\"_blank\">OPT-fine_tuned-FinGPT-CPU</a></strong> to <a href='https://wandb.ai/piyushsonawane10/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/piyushsonawane10/huggingface' target=\"_blank\">https://wandb.ai/piyushsonawane10/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/piyushsonawane10/huggingface/runs/0mqj9454' target=\"_blank\">https://wandb.ai/piyushsonawane10/huggingface/runs/0mqj9454</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "# Initialize the SFTTrainer with the required parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,               # The pre-trained language model to be fine-tuned\n",
        "    args=training_args,        # Training arguments for the trainer\n",
        "    train_dataset=train_dataset,  # Training dataset\n",
        "    eval_dataset=eval_dataset,    # Evaluation dataset\n",
        "    peft_config=loar_config,      # Self-paced fine-tuning (SFT) configuration, possibly for learning rates or other hyperparameters\n",
        "    packing=True,                # Enable packing, a technique for efficient training using smaller batches\n",
        ")\n",
        "\n",
        "print(\"Training...\")\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Merging LoRA and OPT**"
      ],
      "metadata": {
        "id": "eGgy_FpZNvgc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Before conducting inference and observing the results, the final task is to load the LoRA adaptors from the preceding stage and merge them with the base model."
      ],
      "metadata": {
        "id": "iXBQqszyN1O2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the base model (OPT-1.3B) using AutoModelForCausalLM from the Transformers library\n",
        "from transformers import AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Load the pre-trained OPT-1.3B model for causal language modeling\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"facebook/opt-1.3b\",   # Model name or path to the pre-trained model\n",
        "    return_dict=True,       # Return outputs as a dictionary\n",
        "    torch_dtype=torch.bfloat16  # Specify the data type to be used (bfloat16 in this case)\n",
        ")\n",
        "\n",
        "# Load the LoRA adaptors\n",
        "from peft import PeftModel\n",
        "\n",
        "# Load the LoRA model from a specific checkpoint\n",
        "model = PeftModel.from_pretrained(\n",
        "    model,   # The base model (OPT-1.3B) loaded earlier\n",
        "    \"./OPT-fine_tuned-FinGPT-CPU/<desired_checkpoint>/\",   # Path to the directory containing the LoRA checkpoint. This file will be in the formate 'adapter_config.json'.\n",
        ")\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Merge the LoRA adaptors with the base model and unload the adaptors\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Save the merged model for future use\n",
        "model.save_pretrained(\"./OPT-fine_tuned-FinGPT-CPU/merged\")"
      ],
      "metadata": {
        "id": "e9p7ftBb2a0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Inference**"
      ],
      "metadata": {
        "id": "_zFNbF62QiFN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We randomly selected four previously unseen examples from the dataset and provided them as input to both the vanilla base model (OPT-1.3B) and the fine-tuned model in order to contrast their respective responses. The code is relatively straightforward when utilizing the .generate() method from the Transformers library."
      ],
      "metadata": {
        "id": "omz4GyN3Qj8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the input text using the tokenizer\n",
        "inputs = tokenizer(\n",
        "    \"\"\"What is the sentiment of this news? Please choose an answer from\n",
        "    {strong negative/moderately negative/mildly negative/neutral/mildly positive/moderately positive/strong positive},\n",
        "    then provide some short reasons.\\n\\n\n",
        "    Content: UPDATE 1-AstraZeneca sells rare cancer drug to Sanofi for up to S300 mln.\\n\\nSentiment: \"\"\",\n",
        "    return_tensors=\"pt\"  # Return PyTorch tensors\n",
        ").to(\"cuda:0\")  # Move tensors to GPU (cuda:0)\n",
        "\n",
        "# Generate text using the pre-trained language model\n",
        "generation_output = model.generate(\n",
        "    **inputs,  # Pass the tokenized inputs\n",
        "    return_dict_in_generate=True,  # Return outputs as a dictionary\n",
        "    output_scores=True,  # Return the scores of the generated tokens\n",
        "    max_length=256,  # Maximum length of the generated sequence\n",
        "    num_beams=1,  # Number of beams for beam search\n",
        "    do_sample=True,  # Use sampling for generation\n",
        "    repetition_penalty=1.5,  # Repetition penalty to discourage repeated tokens\n",
        "    length_penalty=2.  # Length penalty to encourage longer sequences\n",
        ")\n",
        "\n",
        "# Decode the generated sequence and print the result\n",
        "print(tokenizer.decode(generation_output['sequences'][0]))\n"
      ],
      "metadata": {
        "id": "au612U3KQ_Ik"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzivuEc4o3RsEkeqSqxBCV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}